\subsection{Ranqueamento} \label{subsec:Ranqueamento}
    Devido à limitação dos métodos booleanos de somente retornar resultados conforme a presença ou não dos termos da consulta nos documentos \cite[p.~100]{Manning2008IIR}, foi proposto em 1957 por Luhn e em 1959 por Maron \textit{et al.} uma abordagem de recuperação ranqueada \cite[p.~1446]{Sanderson2012THIRR} a qual, em contraste com recuperação booleana, era baseada nos termos de consulta e estabelecia uma pontuação para cada artigo de modo probabilístico e retornava os artigos de modo ordenado.
    Desta maneira, demonstraram que essa técnica superava a recuperação booleana.
    
    % Explicar o funcionamento de uma recuperação ranqueada, como é feito esse ranqueamento?
    O procedimento fundamental para ranqueamento dos documentos, conforme os termos de consulta, consiste na atribuição de pontuação aos documentos a partir da contabilização do número de aparições (chamada de frequência) de cada um dos termos no documento.
    Essa pontuação é calculada considerando que além da frequência do termo, denotada como $\text{tf}_{\text{\textit{t},\textit{d}}}$ que é o número de ocorrências do termo \textit{t} em um documento \textit{d}, existe também a sua relevância, que depende do número de aparições do termo na coleção de documentos inteira.
    Quanto mais um termo aparece na coleção menos relevante ele é, e este valor de relevância é denotado por $\text{idf}_{\text{\textit{t}}}$ que é o inverso da frequência de um termo \textit{t} em uma coleção de documentos.
    Segundo \citeonline[p.~108]{Manning2008IIR} este valor da relevância é calculado do seguinte modo:

    \begin{equation}
        \label{eq:inverse-document-frequency}
        \text{idf}_{\text{\textit{t}}} = \log{\frac{N}{\text{df}_{\text{\textit{t}}}}},
    \end{equation}
    Onde $N$ é o número total de documentos na coleção, e $\text{df}_{\text{\textit{t}}}$ é a contagem de ocorrências do termo $t$ em toda coleção de documentos.
    
    O valor resultante da relação entre a frequência do termo e o inverso da frequência nos documentos é chamado de $\text{tf-idf}_{\text{\textit{t},\textit{d}}}$ (\textit{term frequency-inverse document frequency}), sendo este valor um dos pesos mais utilizados para ranqueamento \cite[p.~107--110]{Manning2008IIR}, e é calculado  como segue:
    \begin{equation}
        \label{eq:tf-idf}
        \text{tf-idf}_{\text{\textit{t},\textit{d}}}  = \text{tf}_{\text{\textit{t},\textit{d}}} \times \text{idf}_{\text{\textit{t}}}.
    \end{equation}
    
    \input{tex/table/tf-idf-exemplo.tex}
    
    Na Tabela \ref{tab:exemplo-tf-idf} temos um exemplo de cálculo dos valores de tf-idf para posterior cálculo da pontuação para ranqueamento, conforme alguma determinada consulta. 
    A pontuação de um documento \textit{d} é a soma dos pesos de tf-idf de cada termo \textit{t} em \textit{d}, sendo os termos \textit{t} presentes na consulta realizada \cite[p.~109]{Manning2008IIR}, representamos esse cálculo do seguinte modo:
    
    \begin{equation}
        \label{eq:pontuação-simples-tf-idf}
        \text{Pontuação(\textit{q},\textit{d})} = \sum_{\textit{t} \in \textit{q}}^{} \text{tf-idf}_{\text{\textit{t},\textit{d}}}.
    \end{equation}
    
    % Quando uma consulta é feita são utilizados os valores 
    % (onde Pontuação({\textit{auto},\textit{car}},DocX))  
    
    Utilizando a Equação \ref{eq:pontuação-simples-tf-idf} uma consulta com os termos \textit{auto car} retornaria no seu ranqueamento os documentos com a seguinte pontuação, calculamos $\text{Pontuação(\{\textit{auto},\textit{car}\},D\textsubscript{x})}$ para cada documento, por exemplo:
    \begin{itemize}
        \setlength\itemsep{-0.2em}
        \item D\textsubscript{1}: 50,79
        \item D\textsubscript{2}: 75,24
        \item D\textsubscript{3}: 39,60
    \end{itemize}
    
    A ordenação dos documentos apresentados como resultado à consulta \textit{auto car} seria então a seguinte: 1\textordmasculine{} - D\textsubscript{2}; 2\textordmasculine{} - D\textsubscript{1}; e 3\textordmasculine{} - D\textsubscript{3}, que se observamos a Tabela \ref{tab:exemplo-tf-idf} é um bom resultado, já que o D\textsubscript{2} contém uma grande frequência do termo \textit{auto} e o D\textsubscript{3} não possui este termo.
    
    
    Ao longo dos anos foi demonstrada a superioridade da recuperação ranqueada sobre a recuperação booleana \cite{Jones:1981:IRE:539571}, e são as técnicas de recuperação ranqueadas que trazem maior interesse para a área de Mineração de Textos, em específico estamos interessados nos modelos vetoriais e os modelos probabilísticos de RI que são evoluções da recuperação ranqueada.
    % ESTOU PENSANDO EM JÁ CITAR O BM25 no parágrafo acima.
    % -- Em algum ponto mencionar que Recuperação de Informação (Information Retrieval) não deve ser confundido com Procura de Informação (Information Search), pois a Procura de Informação é o campo que estuda a interação das pessoas com sistemas de recuperação de informação.
    
    \subsubsection{Modelo de espaço vetorial} \label{subsubsec:Modelo-espaço-vetorial}
    % O modelo de espaço vetorial.
    
    % Modelo vetorial
    
    %   	Tirado da cabeça
    %     		Utiliza do vetor gerado pelo cálculo dos tf-idf calculados para cada palavra a partir da coleção inteira, e então define que o nível de similaridade entre uma consulta e um arquivo é dado pelo cosseno do ângulo entre os dois vetores.
    
    %   	Do livro do Baeza
        O modelo vetorial surge a partir das limitações do modelo Booleano, que não considera frequência dos termos, e nele são representados um conjunto de documentos num espaço vetorial comum \cite[p.~110]{Manning2008IIR}.
        Oferece a possibilidade de resultados parciais por meio da atribuição de pesos não binários para os termos da consulta e também para os termos presentes nos documentos, que são utilizados para determinar o grau de similaridade entre cada documento armazenado no sistema e uma determinada consulta \cite[p.~77]{Baeza-Yates2011}.
        
        Neste modelo os resultados são apresentados em ordem decrescente de similaridade, considerando a correspondência parcial, e não obrigatoriamente total, com os termos da consulta, provendo assim uma resposta mais precisa para as necessidades de informação do usuário.
        A similaridade nos modelos de espaço vetorial é tratada como uma noção de relevância, e a principal premissa é de que a relevância de um documento em relação a uma consulta está correlacionada com a similaridade entre o documento e consulta \cite[p.~110]{Zhai2016TDMA}.
        
        Segundo \citeonline[p.~77]{Baeza-Yates2011}, os pesos $w_{i,j}$ associados com um par termo-documento são não negativos e não binários.
        Os termos de indexação são considerados mutualmente independentes e são representados como vetores unitários de um espaço $t$-dimensional, aonde $t$ é número total de termos de indexação.
        A representação de um documento $d_j$ e uma consulta $q$ são vetores $t$-dimensionais representados como segue nas Equações \ref{eq:vetor-pesos-documento} e \ref{eq:vetor-pesos-consulta} abaixo:
        \begin{equation}
            \label{eq:vetor-pesos-documento}
    		\vec{d_j} = (w_{1,j}, w_{2,j}, \cdots , w_{t,j})
        \end{equation}
        \begin{equation}
            \label{eq:vetor-pesos-consulta}
    		\vec{q} = (w_{1,q}, w_{2,q}, \cdots , w_{t,q})
        \end{equation}
        
        O valor $\text{tf-idf}_{\text{\textit{t},\textit{d}}}$ (apresentado na Subseção \ref{subsec:Ranqueamento}) é um dos padrões de pesos mais comumente utilizados, sendo aplicado diretamente para os pesos de cada termo do documento $d_j$.
        E como $w_{i,q}$ é o peso associado com o par termo-consulta $(k_i, q)$, a aplicação do $\text{tf-idf}_{\text{\textit{t},\textit{d}}}$ vira $\text{tf-idf}_{k_i\text{,}\textit{q}}$ para os pesos associados à consulta $q$ \cite[p.~77--78]{Baeza-Yates2011}.
        
        Logo, tanto o documento $d_j$ quanto uma consulta $q$ feita pelo usuário são representados como vetores t-dimensionais como ilustrado na Figura \ref{fig:similaridade-cosseno}, posteriormente modulados pelos pesos associados.
        
        \input{tex/figure/cosine-sim.tex}
        
        A avaliação do grau de similaridade entre esses vetores, sendo esta correlação, ou pontuação que o documento $d_j$ vai receber para a consulta $q$, quantificada pelo cosseno do ângulo entre esses dois vetores, conforme demonstra a Equação \ref{eq:pontuação-similaridade-cosseno} \cite[p.~78]{Baeza-Yates2011}.
        \begin{equation}
            \label{eq:pontuação-similaridade-cosseno}
    		\text{Pontuação\_COS}(\vec{d_j}, \vec{q}) = \frac{\vec{d_j} \bullet \vec{q} }{ \norm{\vec{d_j}} \times \norm{\vec{q}} }
        \end{equation}
    
        % falta falar sobre as vantagens e desvantagens citadas pelos autores
        % [falta falar sobre as vantagens e desvantagens citadas pelos autores] feito
        
        \citeonline[p.~79]{Baeza-Yates2011} apontam como vantagens do modelo vetorial:
        \begin{itemize}
            \item \textbf{Melhora da qualidade dos resultados} devido à esquemática de pesos utilizada;
            
            \item Capacidade de \textbf{correspondência parcial} possibilita que documentos \textit{próximos} da consulta sejam retornados;
            
            \item \textbf{Organização dos resultados pelo grau de similaridade com a consulta} devido à fórmula de ranqueamento pelo cosseno dos vetores;
            
            \item \textbf{Normalização dos tamanhos dos documentos} embutida.
        \end{itemize}
        
        Os mesmos autores ainda apontam que a principal desvantagem é a presunção de que os termos indexados são mutualmente independentes, e ressaltam que a tarefa de considerar a dependência dos termos é algo bastante complicado e geram resultados ruins caso não seja feita de modo adequado.
        % Pontuações por Baeza, 1) a esquemática de pesos dos termos melhora a qualidade, 2) sua capacidade de correspondência parcial permite a recuperação de documentos que se aproximam das consultas, 3) o ranqueamento pelo cosseno dos vetores organiza os resultados pelo seu grau de similaridade com a consulta, 4) possui normalização dos tamanhos dos documentos embutida.
        % desvantagem: assume que os termos indexados são mutualmente independentes (porém ele cita que considerar a dependência de termos é uma tarefa complicada que se não for feito do modo apropriado pode levar a resultados ruins).
    
    \subsubsection{Modelo probabilístico}  \label{subsubsec:Modelo-probabilístico}
    % Fazer resumo de como se chegou ao BM25, um método probabilístico criado pelo Kevin Spack e pelo Jones em 85 eu acho, que depois deu início ao BM11 e ao BM15, a junção dos dois deu origem ao BM25, e então o Okapi BM25 surgiu logo após com a adição de técnica de alteração dos pesos dos termos desenvolvida por Okapi.
        Existem diferentes modelos probabilísticos para RI, como por exemplo o modelo linguístico, o modelo de divergência do aleatório (\textit{divergence from randomness}), e o \textit{Framework} de Relevância Probabilística (também chamado de modelo clássico) \cite[p.~87]{Zhai2016TDMA}, que é o mais conhecido por ter dado origem à função BM25 para ranqueamento de documentos \cite[p.~334--335]{robertson_probabilistic_2010} \cite[p.~111]{Zhai2016TDMA}, a qual é nosso foco de discussão.
        BMxx foi a notação utilizada para nomear a funções de ranqueamento do \textit{software} Okapi de RI, desenvolvido na \textit{City, University of London} na década de 90. BM se refere a \textit{Best-matching}, em português melhor correspondência, e a função popularizada como BM25 foi apresentada por \citeonline{Robertson_TREC3_1996}.
        
        % \footnotetext{BMxx foi a notação utilizada para nomear a funções de ranqueamento do software Okapi de RI, desenvolvido na \textit{City, University of London} na década de 90. BM se refere a \textit{Best-matching}, em português melhor correspondência, e a função popularizada como BM25 foi apresentada por \citeonline{Robertson_TREC3_1996}.}
        % As necessidades de informações do usuário são traduzidas em representações de consultas e os documentos de interesse são traduzidos em representações de documentos, e assim os sistemas de RI tentam determinar o quão bem os documentos satisfazem essas necessidades de informação.
        % No entanto, dadas a representações de consulta e de documentos, os sistemas de RI possuem um tanto de incerteza sobre quais documentos tem conteúdo relevante. 
        % Para fazer essa decisão os modelos probabilísticos baseiam-se na teoria probabilística que fundamenta o raciocínio em cima de incertezas \cite[p.~201]{Manning2008IIR}.
        
        Como já abordado, os sistemas de RI tentam determinar o quão bem os documentos satisfazem as necessidades de informação do usuário, porém existe um grau de incerteza sobre quais documentos tem conteúdo relevante. 
        Partindo desse princípio, da existência da incerteza na relevância dos documentos, temos os modelos probabilísticos, que baseiam-se na teoria probabilística que fundamenta o raciocínio em cima de incertezas \cite[p.~201]{Manning2008IIR}. 
        
        Nos modelos probabilísticos, a função de ranqueamento é definida baseada na probabilidade que um documento $d$ é relevante para uma consulta $q$, ou estatisticamente $P(R = 1| d,q)$ onde $R \in \{0, 1\}$ é uma variável binária aleatória que denota a relevância \cite[p.~111--112]{Zhai2016TDMA}, sendo esta a base para o princípio do ranqueamento probabilístico (PRP) \cite[p.~203]{Manning2008IIR}.
        
        O PRP é a base para o chamado \textit{Framework} de Relevância Probabilística o qual, por sua vez, deu origem ao modelo de independência binária (BIM), aos modelos de \textit{feedback} de relevância, ao nosso caso de interesse, o BM25, e também a diversas variações do BM25 \cite[p.~333]{robertson_probabilistic_2010}. 
        
        O BIM faz a implementação do princípio de ranqueamento probabilístico com documentos e consultas sendo representados por vetores binários de incidência dos termos, podendo assim ser comparado ao modelo Booleano \cite[p.~204]{Manning2008IIR}.
        A evolução das implementações dos modelos probabilísticos clássicos levou à função de recuperação conhecida como \textbf{Okapi BM25}, ou simplesmente BM25, que integra os conceitos do modelo vetorial apresentado na Subsubseção \ref{subsubsec:Modelo-espaço-vetorial}, como frequência dos termos, normalização de tamanho, e correspondência parcial.
        Devido à sua similaridade com os modelos vetoriais, alguns autores, como \citeonline[p.~111]{Zhai2016TDMA}, apresentam a função BM25 função junto à dos modelos vetoriais.
        
        Toda teoria probabilística do PRP, que fundamenta os modelos do Framework de Relevância Probabilística, inclusive o BM25, é extensa e portanto ela não será  desenvolvida detalhadamente aqui.
        Abaixo, na Equação \ref{eq:okapi-bm25}, está demonstrada a fórmula do Okapi BM25 similar às fórmulas apresentadas por \citeonline[p.~213--215]{Manning2008IIR} e por \citeonline[p.~107--108]{Zhai2016TDMA}.
        
        \begin{equation}
            \label{eq:okapi-bm25}
    		\text{Pontuação\_BM25}(d_j, q) = \sum_{t \in q} \text{idf}_{\text{\textit{t}}} 
    		\cdot
    		\frac{(k_1 + 1) \text{tf}_{t,d}}{k_1((1-b)+b \times (\frac{L_d}{L_{\text{avg}}})) + \text{tf}_{t,d}} 
        \end{equation}
    
        A fórmula apresentada possui termos já apresentados, como os pesos $\text{tf}_{t,d}$ e $\text{idf}_{\text{\textit{t}}}$, ambos apresentados na Subseção \ref{subsec:Ranqueamento}.
        O termo $\text{tf}_{t,d}$ tem o mesmo significado aqui, é uma contagem do número de ocorrências do termo $t$ no documento $d$, no entanto o termo $\text{idf}_{\text{\textit{t}}}$ na fórmula original do BM25 é conhecido como o peso de Robertson/Spark Jones e pode ser simplificada para a fórmula apresentada na Equação \ref{eq:peso-rsj-adaptado} \cite[p.~347--349]{robertson_probabilistic_2010}.
        \begin{equation}
            \label{eq:peso-rsj-adaptado}
    		\text{idf}_{\text{\textit{t}}} = \log{\frac{N-\text{df}_{\text{\textit{t}}}+\frac{1}{2}}{\text{df}_{\text{\textit{t}}} + \frac{1}{2} }}.
        \end{equation}
        
        É necessário dizer que este termo $\text{idf}_{t}$ pode ser incrementado por outras considerações feitas pelo modelo probabilístico, e também simplificado pela demanda da implementação a ser feita.
        Alguns autores como \citeonline[p.~214]{Manning2008IIR} e \citeonline[p.~108]{Zhai2016TDMA} apresentam este termo como o inverso da frequência do termo $t$, da mesma forma já demonstrada na Equação \ref{eq:inverse-document-frequency}.
        
        A fórmula do BM25 tem dois parâmetros de refinamento, os termos $b$ e $k_1$. 
        O parâmetro $b$ ($0 \leq b \leq 1$) é utilizado para controlar o grau de normalização por tamanho de documento, onde $b=1$ significa uma escalonamento completo e $b=0$ corresponde a não realizá-lo. 
        Esse refinamento toma como ponto de referência o tamanho médio dos documentos na coleção inteira, $L_{\text{avg}}$, em relação ao tamanho do documento $d$, representado por $L_d$.
        
        Já o termo $k_1$ ($0 \leq k_1 \leq \infty$) calibra o efeito da correção de frequência dos termos presente na fórmula, $k_1=0$ significa que não haverá consideração da frequência dos termos, e valores elevados fazem com a fórmula se aproxime da utilização pura do $\text{tf-idf}_{\text{\textit{t},\textit{d}}}$ vista na Equação \ref{eq:tf-idf} \cite[p.~214]{Manning2008IIR}.
        Para consultas extensas, com grande número de termos, \citeonline[p.~214]{Manning2008IIR} afirmam que pode ser adicionado à fórmula um segundo fator $k_3$ que possibilita calibrar o escalonamento de frequência dos termos da consulta, chegando assim à fórmula da Equação \ref{eq:okapi-bm25-tf-consulta} com a simplificação do termo $\text{idf}_{t}$ pela versão simples da Equação \ref{eq:inverse-document-frequency}.
        
        \begin{equation}
            \label{eq:okapi-bm25-tf-consulta}
    		\text{Pontuação\_BM25}(d_j, q) = 
    		\sum_{t \in q} 
    		\Bigg[ \log{\frac{N}{\text{df}_{t}}} \Bigg]
    		\cdot 
    		\frac{(k_1 + 1) \text{tf}_{t,d}}{k_1((1-b)+b \times (\frac{L_d}{L_{\text{avg}}})) + \text{tf}_{t,d}} \cdot 
    		\frac{(k_3+1) \text{tf}_{t,q}}{k_3 + \text{tf}_{t,q}}
        \end{equation}
        %  Here, tftd is the frequency of term t in document d, and L d and L ave are the length of document d and the average document length for the whole collection. The variable k1 is a positive tuning parameter that calibrates the document term frequency scaling. A k 1 value of 0 corresponds to a binary model (no term frequency), and a large value corresponds to using raw term frequency. b is another tuning parameter (0 ≤ b ≤ 1) that determines the scaling by document length: b = 1 corresponds to fully scaling the term weight by the document length, whereas b = 0 corresponds to no length normalization.
        
        Os parâmetros de refinamento devem ser definidos para otimizar o desempenho na recuperação em uma coleção de teste, com a utilização de métodos exaustivos manuais para a busca dos melhores valores, ou com métodos de otimização de funções como por exemplo o \textit{grid search}. 
        Não sendo possível realizar a etapa de otimização, experimentos mostram que é razoável definir tanto $k_1$ como $k_3$ para valores entre $1.2$ e $2$, e definir $b = 0.75$ ou valores entre $0.5$ e $0.8$ \cite[p.~215]{Manning2008IIR} \cite[p.~360--361]{robertson_probabilistic_2010}.
        
    % Probability theory provides a principled foundation for such reasoning under uncertainty
    % Colocar fluxograma do BM25